<!-- <h1 align="center">ğŸƒLeaf</h1>
<div align="center"> -->
  This repo contains my paper reading notes on deep learning and some toy project code, kaggle writeups etc.
  Check the notes at https://bc-li.github.io/paperreading.
</div>

## Paper reading notes
### PHASE #1
| Title                                                        | Field | Time | Report link                       | Time I started | Status      |
| ------------------------------------------------------------ | ----- | ---- | --------------------------------- | ----------- | ----------- |
| [ICCV 2015] Learning Deconvolution Network for Semantic Segmentation | VISION   | 2015 | https://bc-li.github.io/paper/deconvnet | 2021/5/17 | Done |
| [NeurIPS 2017] Attention Is All You Need                     | NLP   | 2017 | https://bc-li.github.io/paper/transformer | 2021/12/11 | Done |
| [NAACL 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | NLP   | 2018 | https://bc-li.github.io/paper/bert         | 2021/12/15 | Done       |
| [NeurIPS 2014] Sequence to Sequence Learning with Neural Networks | NLP   | 2014 | https://bc-li.github.io/paper/seq2seq      | 2022/1/21 | Done  |
| [ICLR 2018] Non-Autoregressive Neural Machine Translation | NLP | 2018 | https://bc-li.github.io/paper/nonauto | 2022/1/24 | Done |
| [ICLR 2019] Parameter-Efficient Transfer Learning for NLP | NLP | 2019 | https://bc-li.github.io/paper/petl | 2022/2/2 | Done |
| [ICLR 2018] Unsupervised Neural Machine Translation | NLP | 2018 | https://bc-li.github.io/paper/unsupervised-NMT | 2022/2/4 | Done |
### PHASE #2
| Title                                                        | Field | Time | Report link                       | Time I started | Status      |
| ------------------------------------------------------------ | ----- | ---- | --------------------------------- | ----------- | ----------- |
| [NeurIPS 2019] Levenshtein Transformer | NLP | 2019 | https://bc-li.github.io/paper/lt | 2022/2/15 | Pending |

### Stack

| Title                                                        | Field | Time | Report link                       | Time I started | Status      |
| ------------------------------------------------------------ | ----- | ---- | --------------------------------- | ----------- | ----------- |
| [SCTS 2020] Pre-trained Models for Natural Language Processing: A Survey | NLP   | 2020 | N/A | N/A | N/A |

> å†™ blog çš„æ—¶å€™å¦‚æœªç‰¹æ®Šè¯´æ˜åˆ™ä¸ºä»çº¦ä¸ºé›¶åŸºç¡€å¼€å§‹ã€‚åœ¨ blog post ä¸­æˆ‘ä¼šæŠŠæˆ‘ä¸ºäº†ç†è§£æ–‡ä¸­ä¸€äº›æ¯”è¾ƒ specific çš„æ¦‚å¿µæ‰¾åˆ°çš„ç›¸å¯¹å®¹æ˜“ç†è§£çš„åŸå‡ºå¤„è´´åˆ°æ–‡ä¸­ï¼Œæ–¹ä¾¿æŸ¥é˜…ï¼Œä¸”ä¸å†é‡å¤é˜è¿°ã€‚

## Future release plan
* Pytorch code corresponding to the papers
* Kaggle writeups
* Course projects (will release at 2022 summer)

## Acknowledgements

https://github.com/mli/paper-reading

https://www.deeplearningbook.org/

